![MÃ©dia PreÃ§o por mÂ² (3 ImobiliÃ¡rias)](Media_Preco_M2_3Imobiliarias.png)

# Scraper para busca do melhor imÃ³vel em sites de imobiliÃ¡rias em FlorianÃ³polis

Um projeto para coletar imÃ³veis residenciais de aluguel no site [ibagy.com.br](https://ibagy.com.br/aluguel/residencial/), implementado com **Scrapy** e AJAX/infinite scroll, e que:

- Pagina dinamicamente via chamadas AJAX (`?reload=0&offset=N`) para capturar todos os blocos de resultados.  
- Deduplica com base no `data-codigo` de cada imÃ³vel.  
- Armazena tudo em `imoveis.json` na raiz do repositÃ³rio.  
- Anexa somente os **novos** imÃ³veis, salvando-os em `novos_imoveis.json`.  
- Envia um **Ãºnico** e-mail resumo listando os imÃ³veis cadastrados em cada execuÃ§Ã£o.  
- Pode ser agendado localmente (via `runner.py`) para rodar a cada 8 horas.  
- **TambÃ©m inclui spiders para as imobiliÃ¡rias Dalton Andrade e PirÃ¢mides, usando a mesma abordagem de infinite scroll e pipeline de notificaÃ§Ã£o.**

---

## ğŸ“ Estrutura do projeto

```
aluguel_fpolis/
â”œâ”€â”€ .env                     # variÃ¡veis de ambiente (SMTP)
â”œâ”€â”€ imoveis.json             # banco mestre de imÃ³veis coletados
â”œâ”€â”€ novos_imoveis.json       # saÃ­da â€œfeedâ€ dos itens novos a cada run
â”œâ”€â”€ runner.py                # script local que agenda o scraper a cada 8h
â”œâ”€â”€ scrapy.cfg               # config do Scrapy apontando para coleta.settings
â””â”€â”€ coleta/                  # pacote Scrapy
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ settings.py          # configuraÃ§Ãµes (pipelines, feeds, delay, etc)
    â”œâ”€â”€ pipelines.py         # dedupe + anexar + notificar por e-mail
    â”œâ”€â”€ spiders/
    â”‚   â”œâ”€â”€ ibagy.py         # spider â€œibagy_com_codâ€ com infinite scroll AJAX
    â”‚   â”œâ”€â”€ daltonandrade_ajax.py    # spider para Dalton Andrade
    â”‚   â””â”€â”€ piramides_ajax.py         # spider para PirÃ¢mides
    â””â”€â”€ utils/
        â””â”€â”€ email_utils.py   # funÃ§Ã£o `enviar_email_resumo(...)`
```

---

## ğŸ”§ InstalaÃ§Ã£o

1. Clone este repositÃ³rio:

   ```bash
   git clone https://github.com/seu-usuario/aluguel_fpolis.git
   cd aluguel_fpolis
   ```

2. Configure as variÃ¡veis de ambiente no arquivo `.env` (na raiz):

   ```ini
   EMAIL_REMETENTE=seu@gmail.com
   EMAIL_PASSWORD=sua_senha_app
   EMAIL_DESTINO=destino@exemplo.com
   ```

3. Instale as dependÃªncias com Poetry:

   ```bash
   poetry install
   ```

---

## ğŸš€ Uso manual

### 1. Listar spiders disponÃ­veis

```bash
cd ~/DS/portfolio/aluguel_fpolis
poetry run scrapy list
```

VocÃª deve ver:

```
ibagy_com_cod
daltonandrade_ajax
piramides_ajax
â€¦
```

### 2. Executar o scraper â€œinfinite scrollâ€ do Ibagy

```bash
poetry run scrapy crawl ibagy_com_cod
```

- Ele irÃ¡:
  - Percorrer a pÃ¡gina inicial (12 imÃ³veis)  
  - Paginar via AJAX (`?reload=0&offset=N`) atÃ© cobrir o total extraÃ­do de `<p.result-totals-phrase>`  
  - Deduplicar itens jÃ¡ em `imoveis.json`  
  - Salvar novos em `novos_imoveis.json`  
  - Anexar esses novos ao `imoveis.json`  
  - Disparar um **Ãºnico e-mail** resumo com tÃ­tulo e link de cada novo imÃ³vel

### 3. Executar tambÃ©m os scrapers das outras imobiliÃ¡rias

- Dalton Andrade:
  ```bash
  poetry run scrapy crawl daltonandrade_ajax
  ```
- PirÃ¢mides:
  ```bash
  poetry run scrapy crawl piramides_ajax
  ```

---

## ğŸ”„ Agendamento local (a cada 8 horas)

Para rodar automaticamente em background:

1. Garanta que o `schedule` estÃ¡ instalado:

   ```bash
   poetry add schedule
   ```

2. Lance o `runner.py` com `nohup`:

   ```bash
   nohup poetry run python runner.py >> runner.log 2>&1 &
   ```

3. Monitore:

   ```bash
   tail -f runner.log
   ```

O `runner.py` faz:

- Primeira execuÃ§Ã£o imediata  
- Agendamento `schedule.every(8).hours.do(run_scraper)`  
- Chama `scrapy crawl ibagy_com_cod` via `subprocess`  
- Loga inÃ­cio, fim e possÃ­veis erros em `runner.log`

---

## ğŸ“§ NotificaÃ§Ãµes por e-mail

O pipeline `ImoveisDedupAndNotifyPipeline` em `coleta/pipelines.py`:

1. LÃª `imoveis.json` na inicializaÃ§Ã£o e carrega todos os `codigo` jÃ¡ processados.  
2. No `process_item` descarta duplicados e armazena sÃ³ os novos.  
3. No `close_spider`:
   - Anexa os novos ao `imoveis.json`  
   - Gera/atualiza `novos_imoveis.json`  
   - Chama `enviar_email_resumo(novos_itens)` em `coleta/utils/email_utils.py`, que envia um **Ãºnico** e-mail listando todos os tÃ­tulos e links dos novos imÃ³veis  

---

## âš™ï¸ ConfiguraÃ§Ãµes principais

- **Pipelines**: habilitado em `coleta/settings.py`
  ```python
  ITEM_PIPELINES = {
      "coleta.pipelines.ImoveisDedupAndNotifyPipeline": 300,
  }
  ```

- **Feeds** (`novos_imoveis.json`):
  ```python
  FEEDS = {
      "novos_imoveis.json": {
          "format": "json",
          "encoding": "utf-8",
          "overwrite": True,
          "indent": 2,
      }
  }
  ```

- **Download delay**: 0.6s entre requisiÃ§Ãµes, para respeitar o servidor.

---

## ğŸ“š ReferÃªncias

- [Scrapy Docs](https://docs.scrapy.org)  
- [Schedule (Python)](https://pypi.org/project/schedule/)  
- [Scrapy AJAX pagination patterns](https://doc.scrapy.org/en/latest/topics/dynamic-content.html)

---

**Este projeto** integra scraping robusto (infinite scroll via AJAX), deduplicaÃ§Ã£o incremental e notificaÃ§Ãµes por e-mail, tudo orquestrado localmente via um simples scheduler em Python. Bom scraping! ğŸš€
